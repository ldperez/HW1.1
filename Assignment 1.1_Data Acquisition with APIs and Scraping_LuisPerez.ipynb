{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b03099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tweepy\n",
      "Version: 4.12.1\n",
      "Summary: Twitter library for Python\n",
      "Home-page: https://www.tweepy.org/\n",
      "Author: Joshua Roesslein\n",
      "Author-email: tweepy@googlegroups.com\n",
      "License: MIT\n",
      "Location: d:\\python\\lib\\site-packages\n",
      "Requires: requests, requests-oauthlib, oauthlib\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "371e8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the twitter section\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729a007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pandas as pd\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a478fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_keys  import api_key, api_key_secret, bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17908d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac0d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.AppAuthHandler(api_key, api_key_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7779b",
   "metadata": {},
   "source": [
    "**Testing the API**\n",
    "\n",
    "The Twitter APIs are quite rich. Let's play around with some of the features before we dive into this section of the assignment. For our testing, it's convenient to have a small data set to play with. We will seed the code with the handle of John Chandler, one of the instructors in this course. His handle is @37chandler. Feel free to use a different handle if you would like to look at someone else's data.\n",
    "\n",
    "We will write code to explore a few aspects of the API:\n",
    "\n",
    "Pull some of the followers @37chandler.\n",
    "\n",
    "Explore response data, which gives us information about Twitter users.\n",
    "\n",
    "Pull the last few tweets by @37chandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1997817",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = \"37chandler\"\n",
    "user_obj = client.get_user(username=handle)\n",
    "\n",
    "followers = client.get_users_followers(\n",
    "    # Learn about user fields here: \n",
    "    # https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user\n",
    "    user_obj.data.id, user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                   \"public_metrics\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3033e08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John chandler lists 'Decatur, GA' as their location.\n",
      " Following: 130, Followers: 10.\n",
      "1609015684796715008\n",
      "Frank P Seidl lists 'Twin Cities, Minnesota USA' as their location.\n",
      " Following: 37863, Followers: 37562.\n",
      "3334369960\n",
      "Roberta lists 'Salinas' as their location.\n",
      " Following: 1895, Followers: 182.\n",
      "1569173836419186689\n",
      "Anna bikes MKE lists 'mke ' as their location.\n",
      " Following: 2287, Followers: 1756.\n",
      "14240035\n",
      "Catherine lists 'San Angelo' as their location.\n",
      " Following: 2193, Followers: 225.\n",
      "1570118574999588864\n"
     ]
    }
   ],
   "source": [
    "num_to_print = 5\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    following_count = user.public_metrics['following_count']\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    print(f\"{user.name} lists '{user.location}' as their location.\")\n",
    "    print(f\" Following: {following_count}, Followers: {followers_count}.\")\n",
    "    print(user.id)\n",
    "    \n",
    "    if idx >= (num_to_print - 1) :\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f06b9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaceConscious\n",
      "{'followers_count': 37562, 'following_count': 37863, 'tweet_count': 13957, 'listed_count': 305}\n"
     ]
    }
   ],
   "source": [
    "max_followers = 0\n",
    "\n",
    "for idx, user in enumerate(followers.data) :\n",
    "    followers_count = user.public_metrics['followers_count']\n",
    "    \n",
    "    if followers_count > max_followers :\n",
    "        max_followers = followers_count\n",
    "        max_follower_user = user\n",
    "\n",
    "        \n",
    "print(max_follower_user)\n",
    "print(max_follower_user.public_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da9d32",
   "metadata": {},
   "source": [
    "Let's pull some more user fields and take a look at them. The fields can be specified in the user_fields argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bc04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.get_user(id=user_obj.data.id,\n",
    "                          user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                                       \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                                       \"verified\",\"public_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49df680a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for username we have 37chandler\n",
      "for description we have He/Him. Data scientist, urban cyclist, educator, erstwhile frisbee player. \n",
      "\n",
      "Â¯\\_(ãƒ„)_/Â¯\n",
      "for location we have MN\n",
      "for public_metrics we have {'followers_count': 185, 'following_count': 592, 'tweet_count': 1049, 'listed_count': 3}\n",
      "for verified we have False\n",
      "for created_at we have 2009-04-18 22:08:22+00:00\n",
      "for id we have 33029025\n",
      "for profile_image_url we have https://pbs.twimg.com/profile_images/2680483898/b30ae76f909352dbae5e371fb1c27454_normal.png\n",
      "for name we have John Chandler\n"
     ]
    }
   ],
   "source": [
    "for field, value in response.data.items() :\n",
    "    print(f\"for {field} we have {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e2ad3",
   "metadata": {},
   "source": [
    "***\n",
    "Now a few questions for you about the user object.\n",
    "\n",
    "Q: How many fields are being returned in the response object?\n",
    "\n",
    "A: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3aab0",
   "metadata": {},
   "source": [
    "***\n",
    "Q: Are any of the fields within the user object non-scalar? (I.e., more complicated than a simple data type like integer, float, string, boolean, etc.)\n",
    "\n",
    "A: NO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376a014",
   "metadata": {},
   "source": [
    "***\n",
    "Q: How many friends, followers, and tweets does this user have?\n",
    "\n",
    "A: followers_count': 183, 'following_count': 592, 'tweet_count': 1049\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a875b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1615204718678007808\n",
      "RT @paulisci: A Brief History of Shorter Work Weeks Are Coming\n",
      "\n",
      "ðŸ§µ\n",
      "\n",
      "1611545485029810180\n",
      "Happy Dia de los Reyes to all who celebrate it. https://t.co/4G7zAuwC70\n",
      "\n",
      "1608230093071212544\n",
      "RT @year_progress: â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“ 99%\n",
      "\n",
      "1606038920604499969\n",
      "RT @CoachBalto: This video is perfect. Parents please watch. Part 1/2 https://t.co/NvcBFmyFPO\n",
      "\n",
      "1602407567036190743\n",
      "RT @LindsayMasland: I had the realization that \"grades are pretend\" the first time I taught (as a TA).  \n",
      "\n",
      "I was grading something with bothâ€¦\n",
      "\n",
      "1598645130075856896\n",
      "RT @marinaendicott: My new favourite lawyerâ€™s letter, just for the sheer joy of the tone.\n",
      "\n",
      "1598156055997222912\n",
      "RT @_TanHo: Hey friends, #AdventOfCode starts TONIGHT! I've organized a friendly leaderboard every year for the #rstats (and friends) commuâ€¦\n",
      "\n",
      "1597746144108740608\n",
      "If you like biking and not getting hit by muederboxes, you should consider one of these. https://t.co/0prMLbvj3b\n",
      "\n",
      "1597734124927995904\n",
      "RT @CraigTheDev: A lot of people argue that AI art isn't theft as it isn't copying the original images but referencing them like a person.â€¦\n",
      "\n",
      "1597641859509415936\n",
      "RT @nytimesbooks: Hereâ€™s our full list of the 10 Best Books of 2022. Learn more about each title here. https://t.co/DtsSSlHyJg https://t.coâ€¦\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.get_users_tweets(user_obj.data.id)\n",
    "\n",
    "# By default, only the ID and text fields of each Tweet will be returned\n",
    "for idx, tweet in enumerate(response.data) :\n",
    "    print(tweet.id)\n",
    "    print(tweet.text)\n",
    "    print()\n",
    "    \n",
    "    if idx > 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78a2ec",
   "metadata": {},
   "source": [
    "***\n",
    "**Pulling Follower Information**\n",
    "\n",
    "In this next section of the assignment, we will pull information about the followers of your two artists. We've seen above how to pull a set of followers using client.get_users_followers. This function has a parameter, max_results, that we can use to change the number of followers that we pull. Unfortunately, we can only pull 1000 followers at a time, which means we will need to handle the pagination of our results.\n",
    "\n",
    "The return object has the .data field, where the results will be found. It also has .meta, which we use to select the next \"page\" in the results using the next_token result. I will illustrate the ideas using our user from above.\n",
    "\n",
    "**Rate Limiting**\n",
    "\n",
    "Twitter limits the rates at which we can pull data, as detailed in this guide. We can make 15 user requests per 15 minutes, meaning that we can pull  users per hour. I illustrate the handling of rate limiting below, though whether or not you hit that part of the code depends on your value of handle.\n",
    "\n",
    "In the below example, I'll pull all the followers, 25 at a time. (We're using 25 to illustrate the idea; when you do this set the value to 1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f83a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_followers = []\n",
    "pulls = 0\n",
    "max_pulls = 100\n",
    "next_token = None\n",
    "\n",
    "while True :\n",
    "\n",
    "    followers = client.get_users_followers(\n",
    "        user_obj.data.id, \n",
    "        max_results=1000, # when you do this for real, set this to 1000!\n",
    "        pagination_token = next_token,\n",
    "        user_fields=[\"created_at\",\"description\",\"location\",\n",
    "                     \"entities\",\"name\",\"pinned_tweet_id\",\"profile_image_url\",\n",
    "                     \"verified\",\"public_metrics\"]\n",
    "    )\n",
    "    pulls += 1\n",
    "    \n",
    "    for follower in followers.data : \n",
    "        follower_row = (follower.id,follower.name,follower.created_at,follower.description)\n",
    "        handle_followers.append(follower_row)\n",
    "    \n",
    "    if 'next_token' in followers.meta and pulls < max_pulls :\n",
    "        next_token = followers.meta['next_token']\n",
    "    else : \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a176591",
   "metadata": {},
   "source": [
    "**Pulling Twitter Data for Your Artists**\n",
    "\n",
    "Now let's take a look at your artists and see how long it is going to take to pull all their followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef72a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take 82.85 hours to pull all 4970910 followers for sanbenito. \n",
      "It would take 893.86 hours to pull all 53631489 followers for shakira. \n"
     ]
    }
   ],
   "source": [
    "artists = dict()\n",
    "\n",
    "for handle in ['sanbenito','shakira'] : \n",
    "    user_obj = client.get_user(username=handle,user_fields=[\"public_metrics\"])\n",
    "    artists[handle] = (user_obj.data.id, \n",
    "                       handle,\n",
    "                       user_obj.data.public_metrics['followers_count'])\n",
    "    \n",
    "\n",
    "for artist, data in artists.items() : \n",
    "    print(f\"It would take {data[2]/(1000*15*4):.2f} hours to pull all {data[2]} followers for {artist}. \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32ed1b",
   "metadata": {},
   "source": [
    "***\n",
    "Depending on what you see in the display above, you may want to limit how many followers you pull. It'd be great to get at least 200,000 per artist.\n",
    "\n",
    "As we pull data for each artist we will write their data to a folder called \"twitter\", so we will make that folder if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e1b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the \"twitter\" folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
    "\n",
    "if not os.path.isdir(\"twitter\") : \n",
    "    #shutil.rmtree(\"twitter/\")\n",
    "    os.mkdir(\"twitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f4357b",
   "metadata": {},
   "source": [
    "In this following cells, build on the above code to pull some of the followers and their data for your two artists. As you pull the data, write the follower ids to a file called [artist name]_followers.txt in the \"twitter\" folder. For instance, for Cher I would create a file named cher_followers.txt. As you pull the data, also store it in an object like a list or a data frame.\n",
    "\n",
    "In addition to creating a file that only has follower IDs in it, you will create a file that includes user data. From the response object please extract and store the following fields:\n",
    "\n",
    "\n",
    "screen_name\n",
    "name\n",
    "id\n",
    "location\n",
    "followers_count\n",
    "friends_count\n",
    "description\n",
    "\n",
    "\n",
    "Store the fields with one user per row in a tab-delimited text file with the name [artist name]_follower_data.txt. For instance, for Cher I would create a file named cher_follower_data.txt.\n",
    "\n",
    "One note: the user's description can have tabs or returns in it, so make sure to clean those out of the description before writing them to the file. I've included some example code to do that below the stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4270ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_followers_to_pull =100*2 # feel free to use this to limit the number of followers you pull.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e610d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
    "    # use `get_users_followers` to pull the follower data requested. \n",
    "\n",
    "    # For each response object, extract the needed fields and store them in a dictionary or\n",
    "    # data frame. \n",
    "\n",
    "    # I recommend writing your results for every response. This isn't the most efficient option\n",
    "    # (since you're opening and closing the file regularly), but it ensures that your \n",
    "    # work is saved in case there is an issue with the API connection. \n",
    "    \n",
    "    # If you've pulled num_followers_to_pull, feel free to break out paged twitter API response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bea77e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'C:\\Users\\Luis Perez\\Documents\\twitter\\sanbenito\\sanbenito_followers.txt' created\n",
      "File 'C:\\Users\\Luis Perez\\Documents\\twitter\\sanbenito\\sanbenito_id_follower_data.txt' created\n",
      "0:00:00.449668\n",
      "File 'C:\\Users\\Luis Perez\\Documents\\twitter\\shakira\\shakira_followers.txt' created\n",
      "File 'C:\\Users\\Luis Perez\\Documents\\twitter\\shakira\\shakira_id_follower_data.txt' created\n",
      "0:00:00.728225\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"twitter\"\n",
    "handles = ['sanbenito','shakira']\n",
    "\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "user_data = dict() \n",
    "followers_data = dict()\n",
    "\n",
    "for handle in handles:\n",
    "    user_data[handle] = [] # will be a list of lists\n",
    "    followers_data[handle] = [] # will be a simple list of IDs\n",
    "    \n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for handle in handles:\n",
    "    \n",
    "    \n",
    "    # Create the output file names \n",
    "    followers_output_file = handle + \"_followers.txt\"\n",
    "    user_data_output_file = handle + \"_id_follower_data.txt\"\n",
    "    \n",
    "    # Using tweepy.Paginator (https://docs.tweepy.org/en/latest/v2_pagination.html), \n",
    "    # use `get_users_followers` to pull the follower data requested. \n",
    "    \n",
    "    user_obj = client.get_user(username = handle)\n",
    "    followers_id = {id: []}\n",
    "        \n",
    "    followers = client.get_users_followers(\n",
    "    user_obj.data.id,pagination_token= next_token,\n",
    "        user_fields = [\"username\", \"description\",\n",
    "                       \"name\", \"id\", \"location\", \"public_metrics\",],)\n",
    "\n",
    "    user_fields = {\"screen_name\": [],\n",
    "                        \"name\": [],\n",
    "                        \"id\": [],\n",
    "                        \"location\": [],\n",
    "                        \"followers_count\": [],\n",
    "                        \"friends_count\":[],\n",
    "                        \"description\": []\n",
    "                       }\n",
    "    # For each response object, extract the needed fields and store them in a dictionary or\n",
    "    # data frame. \n",
    "\n",
    "    for idx, user in enumerate(followers.data):\n",
    "        user_fields[\"screen_name\"].append(user.username),\n",
    "        user_fields[\"name\"].append(user.name),\n",
    "        user_fields[\"id\"].append(user.id),\n",
    "        followers_id[id].append(user.id),\n",
    "        user_fields[\"location\"].append(user.location),\n",
    "        \n",
    "        user_fields[\"description\"].append(user.description),\n",
    "        followers_count= user.public_metrics[\"followers_count\"]\n",
    "        \n",
    "        user_fields[\"followers_count\"].append(followers_count),\n",
    "        following_count = user.public_metrics[\"following_count\"]\n",
    "        \n",
    "        user_fields[\"friends_count\"].append(following_count),\n",
    "          \n",
    "    # I recommend writing your results for every response. This isn't the most efficient option\n",
    "    # (since you're opening and closing the file regularly), but it ensures that your \n",
    "    # work is saved in case there is an issue with the API connection.   \n",
    "\n",
    "    followers_id_df = pd.DataFrame(followers_id)\n",
    "    followers_data_df = pd.DataFrame(user_fields)\n",
    "    \n",
    "    \n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    handle_folder_path = os.path.join(folder_path, handle)\n",
    "    if not os.path.exists(handle_folder_path):\n",
    "        os.mkdir(handle_folder_path)\n",
    "\n",
    "    followers_output_file_path = os.path.join(handle_folder_path, followers_output_file)\n",
    "    user_data_output_file_path = os.path.join(handle_folder_path, user_data_output_file)\n",
    "    \n",
    "\n",
    "    with open(followers_output_file_path, \"w\", encoding='utf-8') as output_file1:\n",
    "        output_file1.write(followers_id_df.to_string())\n",
    "        \n",
    "    with open (user_data_output_file_path, \"w\", encoding='utf-8') as output_file2:\n",
    "        output_file2.write(followers_data_df.to_string())\n",
    "        \n",
    "    print(f\"File '{followers_output_file_path}' created\")\n",
    "    print(f\"File '{user_data_output_file_path}' created\")\n",
    "    \n",
    "    \n",
    "    # Let's see how long it took to grab all follower IDs\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "676c0f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sanbenito we have 100 unique locations.\n"
     ]
    }
   ],
   "source": [
    " user_fields[\"location\"]\n",
    "location_list = user_fields[\"location\"]\n",
    "print(f\"For {artist} we have {len(location_list)} unique locations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49146f",
   "metadata": {},
   "source": [
    "**I was able to print/grab 100 twitter user's information, but when tried to grab more with a loop I would get timeout**\n",
    "\n",
    "**below my attempt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbe511",
   "metadata": {},
   "source": [
    "******\n",
    "**Attempt to run with Loop**\n",
    "```python\n",
    "counter = 0\n",
    "folder_name = \"twitter\"\n",
    "handles = ['sanbenito','shakira']\n",
    "\n",
    "whitespace_pattern = re.compile(r\"\\s+\")\n",
    "\n",
    "user_data = dict() \n",
    "followers_data = dict()\n",
    "\n",
    "for handle in handles:\n",
    "    user_data[handle] = [] # will be a list of lists\n",
    "    followers_data[handle] = [] # will be a simple list of IDs\n",
    "    \n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "for handle in handles:\n",
    "    \n",
    "        \n",
    "    # Create the output file names \n",
    "    followers_output_file = handle + \"_followers.txt\"\n",
    "    user_data_output_file = handle + \"_id_follower_data.txt\"\n",
    "    \n",
    "       \n",
    "    user_obj = client.get_user(username = handle)\n",
    "    followers_id = {id: []}\n",
    "    \n",
    "    while len(followers_id) < num_followers_to_pull:\n",
    "        followers = client.get_users_followers(\n",
    "        user_obj.data.id,\n",
    "        pagination_token= next_token,\n",
    "            user_fields = [\"username\", \"description\", \"name\", \"id\", \"location\", \"public_metrics\",],)\n",
    "\n",
    "       \n",
    "\n",
    "        twitter_data = {\"screen_name\": [],\n",
    "                            \"name\": [],\n",
    "                            \"id\": [],\n",
    "                            \"location\": [],\n",
    "                            \"followers_count\": [],\n",
    "                            \"friends_count\":[],\n",
    "                            \"description\": []\n",
    "                           }\n",
    "        for idx, user in enumerate(followers.data):\n",
    "            twitter_data[\"screen_name\"].append(user.username),\n",
    "            twitter_data[\"name\"].append(user.name),\n",
    "            twitter_data[\"id\"].append(user.id),\n",
    "            followers_id[id].append(user.id),\n",
    "            twitter_data[\"location\"].append(user.location),\n",
    "            twitter_data[\"description\"].append(user.description),\n",
    "            followers_count= user.public_metrics[\"followers_count\"]\n",
    "\n",
    "            twitter_data[\"followers_count\"].append(followers_count),\n",
    "            following_count = user.public_metrics[\"following_count\"]\n",
    "\n",
    "            twitter_data[\"friends_count\"].append(following_count),\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        followers_id_df = pd.DataFrame(followers_id)\n",
    "        followers_data_df = pd.DataFrame(twitter_data)\n",
    "\n",
    "        append new pages\n",
    "        updated_followers_id.append(followers_id_df)\n",
    "        updated_followers_data.append(followers_data_df)\n",
    "    \n",
    "    \n",
    "    if len(followers_id_df) >= num_followers_to_pull or next_token is None:\n",
    "        break\n",
    "    \n",
    "\n",
    "    folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "\n",
    "    handle_folder_path = os.path.join(folder_path, handle)\n",
    "    if not os.path.exists(handle_folder_path):\n",
    "        os.mkdir(handle_folder_path)\n",
    "\n",
    "    followers_output_file_path = os.path.join(handle_folder_path, followers_output_file)\n",
    "    user_data_output_file_path = os.path.join(handle_folder_path, user_data_output_file)\n",
    "    \n",
    "\n",
    "    with open(followers_output_file_path, \"w\", encoding='utf-8') as output_file1:\n",
    "        output_file1.write(followers_id_df.to_string())\n",
    "       \n",
    "        \n",
    "    #with open (followers_output_file_path, \"w\", encoding='utf-8') as output_file2:\n",
    "        #output_file2.write(followers_data_df.to_string())\n",
    "        \n",
    "    with open (user_data_output_file_path, \"w\", encoding='utf-8') as output_file2:\n",
    "        output_file2.write(followers_data_df.to_string())\n",
    "        \n",
    "    print(f\"File '{followers_output_file_path}' created\")\n",
    "    print(f\"File '{user_data_output_file_path}' created\")\n",
    "    \n",
    "    \n",
    "    # Let's see how long it took to grab all follower IDs\n",
    "    end_time = datetime.datetime.now()\n",
    "    print(end_time - start_time)\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e3638",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15ed8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#artists = {'robyn':\"https://www.azlyrics.com/r/robyn.html\",\n",
    "  #         'cher':\"https://www.azlyrics.com/c/cher.html\"} \n",
    "\n",
    "artists = {'badbunny':\"https://www.azlyrics.com/b/badbunny.html\",\n",
    "           'shakira':\"https://www.azlyrics.com/s/shakira.html\"}\n",
    "\n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1502102",
   "metadata": {},
   "source": [
    "A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to a Tom Jones song.)\n",
    "\n",
    "Whenever you call requests.get to retrieve a page, put a time.sleep(5 + 10*random.random()) on the next line. This will help you not to get blocked. If you do get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78367b76",
   "metadata": {},
   "source": [
    "Part 1: Finding Links to Songs Lyrics\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages.\n",
    "***\n",
    "Q: Take a look at the robots.txt page on www.azlyrics.com. (You can read more about these pages here.) Is the scraping we are about to do allowed or disallowed by this page? How do you know?\n",
    "\n",
    "A: There are multiple ways to check the website robots.txt file, one way is to add \"/robots.txt at the end of the website. For azlyrics.com we are allowed to scrape except for these folders \"/lyricsdb/ and /song/\". \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ece9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items():\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "\n",
    "    # now extract the links to lyrics pages from this page\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "    # find all the links on the page\n",
    "    links = soup.find_all('a', href=lambda x: x and x.startswith('/lyrics'))\n",
    "    links = [link.get('href') for link in links]\n",
    "   \n",
    "    # store the links in the dictionary\n",
    "    lyrics_pages[artist] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1906c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13027388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For badbunny we have 143.\n",
      "The full pull will take for this artist will take 0.4 hours.\n",
      "For shakira we have 157.\n",
      "The full pull will take for this artist will take 0.44 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ec2c8",
   "metadata": {},
   "source": [
    "**Part 2: Pulling Lyrics**\n",
    "\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part.\n",
    "\n",
    "1.Create an empty folder in our repo called \"lyrics\".\n",
    "\n",
    "2.Iterate over the artists in lyrics_pages.\n",
    "\n",
    "3.Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have lyrics/cher/ in your repo.\n",
    "\n",
    "4.Iterate over the pages.\n",
    "\n",
    "5.Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "\n",
    "6.Use the function below, generate_filename_from_url, to create a filename based on the lyrics page, then write the lyrics to a text file with that name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4de44817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "056909c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
    "\n",
    "if os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "\n",
    "os.mkdir(\"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8580674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 pages scraped\n",
      "Elapsed time: 7.157498121261597\n",
      "2 pages scraped\n",
      "Elapsed time: 13.636198997497559\n",
      "3 pages scraped\n",
      "Elapsed time: 28.105494737625122\n",
      "4 pages scraped\n",
      "Elapsed time: 35.974318742752075\n",
      "5 pages scraped\n",
      "Elapsed time: 42.315120220184326\n",
      "6 pages scraped\n",
      "Elapsed time: 52.01504135131836\n",
      "7 pages scraped\n",
      "Elapsed time: 61.95275640487671\n",
      "8 pages scraped\n",
      "Elapsed time: 70.26995992660522\n",
      "9 pages scraped\n",
      "Elapsed time: 80.73064970970154\n",
      "10 pages scraped\n",
      "Elapsed time: 92.25739407539368\n",
      "11 pages scraped\n",
      "Elapsed time: 100.60347175598145\n",
      "12 pages scraped\n",
      "Elapsed time: 111.7222192287445\n",
      "13 pages scraped\n",
      "Elapsed time: 127.085280418396\n",
      "14 pages scraped\n",
      "Elapsed time: 141.03794813156128\n",
      "15 pages scraped\n",
      "Elapsed time: 155.30422830581665\n",
      "16 pages scraped\n",
      "Elapsed time: 169.69862341880798\n",
      "17 pages scraped\n",
      "Elapsed time: 175.97265934944153\n",
      "18 pages scraped\n",
      "Elapsed time: 185.95186161994934\n",
      "19 pages scraped\n",
      "Elapsed time: 193.76842403411865\n",
      "20 pages scraped\n",
      "Elapsed time: 204.6372787952423\n",
      "21 pages scraped\n",
      "Elapsed time: 216.57269954681396\n",
      "22 pages scraped\n",
      "Elapsed time: 225.09260416030884\n",
      "23 pages scraped\n",
      "Elapsed time: 237.70972657203674\n",
      "24 pages scraped\n",
      "Elapsed time: 250.96375632286072\n",
      "25 pages scraped\n",
      "Elapsed time: 259.2859888076782\n",
      "26 pages scraped\n",
      "Elapsed time: 270.69543623924255\n",
      "27 pages scraped\n",
      "Elapsed time: 276.78977251052856\n",
      "28 pages scraped\n",
      "Elapsed time: 289.7905662059784\n",
      "29 pages scraped\n",
      "Elapsed time: 304.01483821868896\n",
      "30 pages scraped\n",
      "Elapsed time: 318.8894419670105\n",
      "31 pages scraped\n",
      "Elapsed time: 328.05308771133423\n",
      "32 pages scraped\n",
      "Elapsed time: 341.4574284553528\n",
      "33 pages scraped\n",
      "Elapsed time: 352.7122676372528\n",
      "34 pages scraped\n",
      "Elapsed time: 361.9344127178192\n",
      "35 pages scraped\n",
      "Elapsed time: 374.46336245536804\n",
      "36 pages scraped\n",
      "Elapsed time: 387.94989490509033\n",
      "37 pages scraped\n",
      "Elapsed time: 398.5791206359863\n",
      "38 pages scraped\n",
      "Elapsed time: 411.8904621601105\n",
      "39 pages scraped\n",
      "Elapsed time: 422.0960762500763\n",
      "40 pages scraped\n",
      "Elapsed time: 432.94901394844055\n",
      "41 pages scraped\n",
      "Elapsed time: 447.375848531723\n",
      "42 pages scraped\n",
      "Elapsed time: 454.97642040252686\n",
      "43 pages scraped\n",
      "Elapsed time: 469.52837467193604\n",
      "44 pages scraped\n",
      "Elapsed time: 475.5367865562439\n",
      "45 pages scraped\n",
      "Elapsed time: 481.3566644191742\n",
      "46 pages scraped\n",
      "Elapsed time: 489.92048501968384\n",
      "47 pages scraped\n",
      "Elapsed time: 501.80902767181396\n",
      "48 pages scraped\n",
      "Elapsed time: 516.1513304710388\n",
      "49 pages scraped\n",
      "Elapsed time: 522.1297404766083\n",
      "50 pages scraped\n",
      "Elapsed time: 529.5752670764923\n",
      "51 pages scraped\n",
      "Elapsed time: 541.2359368801117\n",
      "52 pages scraped\n",
      "Elapsed time: 553.1054563522339\n",
      "53 pages scraped\n",
      "Elapsed time: 567.4483242034912\n",
      "54 pages scraped\n",
      "Elapsed time: 580.8536944389343\n",
      "55 pages scraped\n",
      "Elapsed time: 592.2267882823944\n",
      "56 pages scraped\n",
      "Elapsed time: 606.8625586032867\n",
      "57 pages scraped\n",
      "Elapsed time: 612.7155137062073\n",
      "58 pages scraped\n",
      "Elapsed time: 621.7323472499847\n",
      "59 pages scraped\n",
      "Elapsed time: 633.0438063144684\n",
      "60 pages scraped\n",
      "Elapsed time: 640.3606882095337\n",
      "61 pages scraped\n",
      "Elapsed time: 654.5772337913513\n",
      "62 pages scraped\n",
      "Elapsed time: 661.6071717739105\n",
      "63 pages scraped\n",
      "Elapsed time: 668.7188704013824\n",
      "64 pages scraped\n",
      "Elapsed time: 680.4472966194153\n",
      "65 pages scraped\n",
      "Elapsed time: 693.108470916748\n",
      "66 pages scraped\n",
      "Elapsed time: 702.8823223114014\n",
      "67 pages scraped\n",
      "Elapsed time: 715.0139801502228\n",
      "68 pages scraped\n",
      "Elapsed time: 729.8484792709351\n",
      "69 pages scraped\n",
      "Elapsed time: 739.1576309204102\n",
      "70 pages scraped\n",
      "Elapsed time: 747.1824522018433\n",
      "71 pages scraped\n",
      "Elapsed time: 757.122888803482\n",
      "72 pages scraped\n",
      "Elapsed time: 768.4614534378052\n",
      "73 pages scraped\n",
      "Elapsed time: 777.4186236858368\n",
      "74 pages scraped\n",
      "Elapsed time: 792.6468796730042\n",
      "75 pages scraped\n",
      "Elapsed time: 804.223637342453\n",
      "76 pages scraped\n",
      "Elapsed time: 813.2905900478363\n",
      "77 pages scraped\n",
      "Elapsed time: 826.0260694026947\n",
      "78 pages scraped\n",
      "Elapsed time: 834.433895111084\n",
      "79 pages scraped\n",
      "Elapsed time: 843.5596008300781\n",
      "80 pages scraped\n",
      "Elapsed time: 853.5494563579559\n",
      "81 pages scraped\n",
      "Elapsed time: 859.8571999073029\n",
      "82 pages scraped\n",
      "Elapsed time: 868.610326051712\n",
      "83 pages scraped\n",
      "Elapsed time: 883.1630024909973\n",
      "84 pages scraped\n",
      "Elapsed time: 897.769543170929\n",
      "85 pages scraped\n",
      "Elapsed time: 909.4046068191528\n",
      "86 pages scraped\n",
      "Elapsed time: 917.6721177101135\n",
      "87 pages scraped\n",
      "Elapsed time: 924.4831492900848\n",
      "88 pages scraped\n",
      "Elapsed time: 934.8539936542511\n",
      "89 pages scraped\n",
      "Elapsed time: 942.1049473285675\n",
      "90 pages scraped\n",
      "Elapsed time: 953.6159627437592\n",
      "91 pages scraped\n",
      "Elapsed time: 962.5826241970062\n",
      "92 pages scraped\n",
      "Elapsed time: 975.5650579929352\n",
      "93 pages scraped\n",
      "Elapsed time: 986.9353909492493\n",
      "94 pages scraped\n",
      "Elapsed time: 1001.4734091758728\n",
      "95 pages scraped\n",
      "Elapsed time: 1015.5247690677643\n",
      "96 pages scraped\n",
      "Elapsed time: 1023.9086878299713\n",
      "97 pages scraped\n",
      "Elapsed time: 1036.0854935646057\n",
      "98 pages scraped\n",
      "Elapsed time: 1042.4936201572418\n",
      "99 pages scraped\n",
      "Elapsed time: 1052.239599943161\n",
      "100 pages scraped\n",
      "Elapsed time: 1064.3869774341583\n",
      "101 pages scraped\n",
      "Elapsed time: 1070.53338098526\n",
      "102 pages scraped\n",
      "Elapsed time: 1077.2647159099579\n",
      "103 pages scraped\n",
      "Elapsed time: 1087.125179052353\n",
      "104 pages scraped\n",
      "Elapsed time: 1097.1430587768555\n",
      "105 pages scraped\n",
      "Elapsed time: 1107.6915283203125\n",
      "106 pages scraped\n",
      "Elapsed time: 1117.798166513443\n",
      "107 pages scraped\n",
      "Elapsed time: 1131.3302881717682\n",
      "108 pages scraped\n",
      "Elapsed time: 1145.4701356887817\n",
      "109 pages scraped\n",
      "Elapsed time: 1154.9865217208862\n",
      "110 pages scraped\n",
      "Elapsed time: 1169.1222105026245\n",
      "111 pages scraped\n",
      "Elapsed time: 1180.9334120750427\n",
      "112 pages scraped\n",
      "Elapsed time: 1187.2575056552887\n",
      "113 pages scraped\n",
      "Elapsed time: 1202.0187585353851\n",
      "114 pages scraped\n",
      "Elapsed time: 1215.2688331604004\n",
      "115 pages scraped\n",
      "Elapsed time: 1220.7766416072845\n",
      "116 pages scraped\n",
      "Elapsed time: 1227.2956583499908\n",
      "117 pages scraped\n",
      "Elapsed time: 1242.6882202625275\n",
      "118 pages scraped\n",
      "Elapsed time: 1250.4425265789032\n",
      "119 pages scraped\n",
      "Elapsed time: 1258.8988409042358\n",
      "120 pages scraped\n",
      "Elapsed time: 1273.7829167842865\n",
      "121 pages scraped\n",
      "Elapsed time: 1279.5834398269653\n",
      "122 pages scraped\n",
      "Elapsed time: 1294.6535861492157\n",
      "123 pages scraped\n",
      "Elapsed time: 1303.6467823982239\n",
      "124 pages scraped\n",
      "Elapsed time: 1318.4506533145905\n",
      "125 pages scraped\n",
      "Elapsed time: 1329.1829764842987\n",
      "126 pages scraped\n",
      "Elapsed time: 1335.4063503742218\n",
      "127 pages scraped\n",
      "Elapsed time: 1349.7364938259125\n",
      "128 pages scraped\n",
      "Elapsed time: 1356.7007856369019\n",
      "129 pages scraped\n",
      "Elapsed time: 1367.3177466392517\n",
      "130 pages scraped\n",
      "Elapsed time: 1375.649305820465\n",
      "131 pages scraped\n",
      "Elapsed time: 1382.8968787193298\n",
      "132 pages scraped\n",
      "Elapsed time: 1395.5636339187622\n",
      "133 pages scraped\n",
      "Elapsed time: 1409.804054260254\n",
      "134 pages scraped\n",
      "Elapsed time: 1415.7749071121216\n",
      "135 pages scraped\n",
      "Elapsed time: 1424.7792420387268\n",
      "136 pages scraped\n",
      "Elapsed time: 1436.3296558856964\n",
      "137 pages scraped\n",
      "Elapsed time: 1446.2043192386627\n",
      "138 pages scraped\n",
      "Elapsed time: 1458.0857021808624\n",
      "139 pages scraped\n",
      "Elapsed time: 1464.7714631557465\n",
      "140 pages scraped\n",
      "Elapsed time: 1474.5785491466522\n",
      "141 pages scraped\n",
      "Elapsed time: 1482.1668858528137\n",
      "142 pages scraped\n",
      "Elapsed time: 1496.6958734989166\n",
      "143 pages scraped\n",
      "Elapsed time: 1506.2514572143555\n",
      "144 pages scraped\n",
      "Elapsed time: 1517.0615403652191\n",
      "145 pages scraped\n",
      "Elapsed time: 1524.2375202178955\n",
      "146 pages scraped\n",
      "Elapsed time: 1535.1758472919464\n",
      "147 pages scraped\n",
      "Elapsed time: 1541.6966004371643\n",
      "148 pages scraped\n",
      "Elapsed time: 1555.019341468811\n",
      "149 pages scraped\n",
      "Elapsed time: 1566.5372943878174\n",
      "150 pages scraped\n",
      "Elapsed time: 1572.3791809082031\n",
      "151 pages scraped\n",
      "Elapsed time: 1585.9612295627594\n",
      "152 pages scraped\n",
      "Elapsed time: 1595.7735061645508\n",
      "153 pages scraped\n",
      "Elapsed time: 1609.144912481308\n",
      "154 pages scraped\n",
      "Elapsed time: 1621.0549132823944\n",
      "155 pages scraped\n",
      "Elapsed time: 1628.2885477542877\n",
      "156 pages scraped\n",
      "Elapsed time: 1638.337656736374\n",
      "157 pages scraped\n",
      "Elapsed time: 1645.213930606842\n",
      "158 pages scraped\n",
      "Elapsed time: 1652.2654774188995\n",
      "159 pages scraped\n",
      "Elapsed time: 1662.5952587127686\n",
      "160 pages scraped\n",
      "Elapsed time: 1675.852555513382\n",
      "161 pages scraped\n",
      "Elapsed time: 1691.1213989257812\n",
      "162 pages scraped\n",
      "Elapsed time: 1702.2531242370605\n",
      "163 pages scraped\n",
      "Elapsed time: 1713.4577684402466\n",
      "164 pages scraped\n",
      "Elapsed time: 1725.3992838859558\n",
      "165 pages scraped\n",
      "Elapsed time: 1732.0568380355835\n",
      "166 pages scraped\n",
      "Elapsed time: 1745.4154343605042\n",
      "167 pages scraped\n",
      "Elapsed time: 1755.9361770153046\n",
      "168 pages scraped\n",
      "Elapsed time: 1771.2554910182953\n",
      "169 pages scraped\n",
      "Elapsed time: 1777.5407240390778\n",
      "170 pages scraped\n",
      "Elapsed time: 1785.2417340278625\n",
      "171 pages scraped\n",
      "Elapsed time: 1793.504567861557\n",
      "172 pages scraped\n",
      "Elapsed time: 1802.6132292747498\n",
      "173 pages scraped\n",
      "Elapsed time: 1815.1502990722656\n",
      "174 pages scraped\n",
      "Elapsed time: 1823.944324016571\n",
      "175 pages scraped\n",
      "Elapsed time: 1838.2858033180237\n",
      "176 pages scraped\n",
      "Elapsed time: 1847.1211478710175\n",
      "177 pages scraped\n",
      "Elapsed time: 1855.016316652298\n",
      "178 pages scraped\n",
      "Elapsed time: 1864.0166511535645\n",
      "179 pages scraped\n",
      "Elapsed time: 1871.4563627243042\n",
      "180 pages scraped\n",
      "Elapsed time: 1880.2079403400421\n",
      "181 pages scraped\n",
      "Elapsed time: 1894.3175873756409\n",
      "182 pages scraped\n",
      "Elapsed time: 1903.081093788147\n",
      "183 pages scraped\n",
      "Elapsed time: 1910.9741563796997\n",
      "184 pages scraped\n",
      "Elapsed time: 1916.7798521518707\n",
      "185 pages scraped\n",
      "Elapsed time: 1930.1585357189178\n",
      "186 pages scraped\n",
      "Elapsed time: 1938.3197779655457\n",
      "187 pages scraped\n",
      "Elapsed time: 1950.172835111618\n",
      "188 pages scraped\n",
      "Elapsed time: 1958.1167948246002\n",
      "189 pages scraped\n",
      "Elapsed time: 1969.3730292320251\n",
      "190 pages scraped\n",
      "Elapsed time: 1980.8722870349884\n",
      "191 pages scraped\n",
      "Elapsed time: 1995.0153760910034\n",
      "192 pages scraped\n",
      "Elapsed time: 2004.3287222385406\n",
      "193 pages scraped\n",
      "Elapsed time: 2014.7381882667542\n",
      "194 pages scraped\n",
      "Elapsed time: 2029.8881666660309\n",
      "195 pages scraped\n",
      "Elapsed time: 2044.3383903503418\n",
      "196 pages scraped\n",
      "Elapsed time: 2053.889646291733\n",
      "197 pages scraped\n",
      "Elapsed time: 2063.7807285785675\n",
      "198 pages scraped\n",
      "Elapsed time: 2070.503745317459\n",
      "199 pages scraped\n",
      "Elapsed time: 2077.902860403061\n",
      "200 pages scraped\n",
      "Elapsed time: 2091.13138794899\n",
      "201 pages scraped\n",
      "Elapsed time: 2099.6423478126526\n",
      "202 pages scraped\n",
      "Elapsed time: 2108.156642436981\n",
      "203 pages scraped\n",
      "Elapsed time: 2115.1322157382965\n",
      "204 pages scraped\n",
      "Elapsed time: 2126.2396972179413\n",
      "205 pages scraped\n",
      "Elapsed time: 2137.944536447525\n",
      "206 pages scraped\n",
      "Elapsed time: 2146.5106585025787\n",
      "207 pages scraped\n",
      "Elapsed time: 2161.406866312027\n",
      "208 pages scraped\n",
      "Elapsed time: 2170.594672679901\n",
      "209 pages scraped\n",
      "Elapsed time: 2182.001900434494\n",
      "210 pages scraped\n",
      "Elapsed time: 2199.4870896339417\n",
      "211 pages scraped\n",
      "Elapsed time: 2209.9720537662506\n",
      "212 pages scraped\n",
      "Elapsed time: 2221.1741015911102\n",
      "213 pages scraped\n",
      "Elapsed time: 2229.524795770645\n",
      "214 pages scraped\n",
      "Elapsed time: 2241.5805344581604\n",
      "215 pages scraped\n",
      "Elapsed time: 2252.845415830612\n",
      "216 pages scraped\n",
      "Elapsed time: 2261.4883313179016\n",
      "217 pages scraped\n",
      "Elapsed time: 2269.0650703907013\n",
      "218 pages scraped\n",
      "Elapsed time: 2283.846529483795\n",
      "219 pages scraped\n",
      "Elapsed time: 2291.08118391037\n",
      "220 pages scraped\n",
      "Elapsed time: 2299.7061216831207\n",
      "221 pages scraped\n",
      "Elapsed time: 2313.831351518631\n",
      "222 pages scraped\n",
      "Elapsed time: 2320.644162416458\n",
      "223 pages scraped\n",
      "Elapsed time: 2332.51838350296\n",
      "224 pages scraped\n",
      "Elapsed time: 2339.689208984375\n",
      "225 pages scraped\n",
      "Elapsed time: 2346.4701063632965\n",
      "226 pages scraped\n",
      "Elapsed time: 2357.5763800144196\n",
      "227 pages scraped\n",
      "Elapsed time: 2364.4709601402283\n",
      "228 pages scraped\n",
      "Elapsed time: 2373.3950815200806\n",
      "229 pages scraped\n",
      "Elapsed time: 2383.677587032318\n",
      "230 pages scraped\n",
      "Elapsed time: 2394.488678455353\n",
      "231 pages scraped\n",
      "Elapsed time: 2400.847677230835\n",
      "232 pages scraped\n",
      "Elapsed time: 2411.7066667079926\n",
      "233 pages scraped\n",
      "Elapsed time: 2424.719870328903\n",
      "234 pages scraped\n",
      "Elapsed time: 2432.5977771282196\n",
      "235 pages scraped\n",
      "Elapsed time: 2444.3164455890656\n",
      "236 pages scraped\n",
      "Elapsed time: 2451.846309185028\n",
      "237 pages scraped\n",
      "Elapsed time: 2465.033046722412\n",
      "238 pages scraped\n",
      "Elapsed time: 2480.5794768333435\n",
      "239 pages scraped\n",
      "Elapsed time: 2489.253310918808\n",
      "240 pages scraped\n",
      "Elapsed time: 2499.330338001251\n",
      "241 pages scraped\n",
      "Elapsed time: 2513.0985226631165\n",
      "242 pages scraped\n",
      "Elapsed time: 2525.0934493541718\n",
      "243 pages scraped\n",
      "Elapsed time: 2539.4560718536377\n",
      "244 pages scraped\n",
      "Elapsed time: 2554.4708955287933\n",
      "245 pages scraped\n",
      "Elapsed time: 2564.346489429474\n",
      "246 pages scraped\n",
      "Elapsed time: 2578.721051454544\n",
      "247 pages scraped\n",
      "Elapsed time: 2586.8632802963257\n",
      "248 pages scraped\n",
      "Elapsed time: 2595.314709186554\n",
      "249 pages scraped\n",
      "Elapsed time: 2603.455939769745\n",
      "250 pages scraped\n",
      "Elapsed time: 2616.362428665161\n",
      "251 pages scraped\n",
      "Elapsed time: 2631.738286972046\n",
      "252 pages scraped\n",
      "Elapsed time: 2647.021420955658\n",
      "253 pages scraped\n",
      "Elapsed time: 2661.1765987873077\n",
      "254 pages scraped\n",
      "Elapsed time: 2674.3323929309845\n",
      "255 pages scraped\n",
      "Elapsed time: 2682.034830570221\n",
      "256 pages scraped\n",
      "Elapsed time: 2693.97190618515\n",
      "257 pages scraped\n",
      "Elapsed time: 2702.6117753982544\n",
      "258 pages scraped\n",
      "Elapsed time: 2716.7709155082703\n",
      "259 pages scraped\n",
      "Elapsed time: 2722.798796415329\n",
      "260 pages scraped\n",
      "Elapsed time: 2731.8186779022217\n",
      "261 pages scraped\n",
      "Elapsed time: 2738.315306663513\n",
      "262 pages scraped\n",
      "Elapsed time: 2744.3950493335724\n",
      "263 pages scraped\n",
      "Elapsed time: 2755.972093105316\n",
      "264 pages scraped\n",
      "Elapsed time: 2768.7698724269867\n",
      "265 pages scraped\n",
      "Elapsed time: 2781.0960426330566\n",
      "266 pages scraped\n",
      "Elapsed time: 2792.519301176071\n",
      "267 pages scraped\n",
      "Elapsed time: 2799.3995594978333\n",
      "268 pages scraped\n",
      "Elapsed time: 2811.8469109535217\n",
      "269 pages scraped\n",
      "Elapsed time: 2818.942938566208\n",
      "270 pages scraped\n",
      "Elapsed time: 2831.130034685135\n",
      "271 pages scraped\n",
      "Elapsed time: 2845.947503566742\n",
      "272 pages scraped\n",
      "Elapsed time: 2855.4224972724915\n",
      "273 pages scraped\n",
      "Elapsed time: 2866.238794565201\n",
      "274 pages scraped\n",
      "Elapsed time: 2876.184155702591\n",
      "275 pages scraped\n",
      "Elapsed time: 2888.6032404899597\n",
      "276 pages scraped\n",
      "Elapsed time: 2902.8323357105255\n",
      "277 pages scraped\n",
      "Elapsed time: 2913.0725042819977\n",
      "278 pages scraped\n",
      "Elapsed time: 2925.693593978882\n",
      "279 pages scraped\n",
      "Elapsed time: 2937.7578349113464\n",
      "280 pages scraped\n",
      "Elapsed time: 2952.7388257980347\n",
      "281 pages scraped\n",
      "Elapsed time: 2965.879894733429\n",
      "282 pages scraped\n",
      "Elapsed time: 2972.038960456848\n",
      "283 pages scraped\n",
      "Elapsed time: 2985.689994573593\n",
      "284 pages scraped\n",
      "Elapsed time: 2995.7189445495605\n",
      "285 pages scraped\n",
      "Elapsed time: 3004.8461558818817\n",
      "286 pages scraped\n",
      "Elapsed time: 3020.314924478531\n",
      "287 pages scraped\n",
      "Elapsed time: 3035.8124153614044\n",
      "288 pages scraped\n",
      "Elapsed time: 3043.1537861824036\n",
      "289 pages scraped\n",
      "Elapsed time: 3055.1150176525116\n",
      "290 pages scraped\n",
      "Elapsed time: 3063.5986816883087\n",
      "291 pages scraped\n",
      "Elapsed time: 3076.144233942032\n",
      "292 pages scraped\n",
      "Elapsed time: 3091.509114265442\n",
      "293 pages scraped\n",
      "Elapsed time: 3098.1711490154266\n",
      "294 pages scraped\n",
      "Elapsed time: 3109.6520845890045\n",
      "295 pages scraped\n",
      "Elapsed time: 3116.515950202942\n",
      "296 pages scraped\n",
      "Elapsed time: 3124.447376012802\n",
      "297 pages scraped\n",
      "Elapsed time: 3134.1813027858734\n",
      "298 pages scraped\n",
      "Elapsed time: 3146.670647621155\n",
      "299 pages scraped\n",
      "Elapsed time: 3153.022053718567\n",
      "300 pages scraped\n",
      "Elapsed time: 3163.523989677429\n"
     ]
    }
   ],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0\n",
    "\n",
    "\n",
    "for artist in lyrics_pages:\n",
    "    \n",
    "    # create a folder with the artist name\n",
    "    sub_artist_folder = os.path.join(\"lyrics\", artist)  \n",
    "    if not os.path.exists(sub_artist_folder):\n",
    "        os.makedirs(sub_artist_folder)\n",
    "    \n",
    "    \n",
    "    for lyrics_page in lyrics_pages[artist]:\n",
    "        url = url_stub + lyrics_page\n",
    "        r = requests.get(url)\n",
    "        time.sleep(5+10*random.random())\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        lyrics= soup.find(class_='col-xs-12 col-lg-8 text-center').text\n",
    "    \n",
    "        \n",
    "        div = soup.find('div', class_='col-xs-12 col-lg-8 text-center')\n",
    "        long_way = div.find_all('b')\n",
    "        title =long_way[1].text\n",
    "        \n",
    "\n",
    "        #lyrics = re.sub(r\"\\s+\", \"\", lyrics, flags=re.UNICODE)\n",
    "        \n",
    "        \n",
    "        # 5. Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url` to generate the filename. \n",
    "        filename = generate_filename_from_link(lyrics_page)\n",
    "        \n",
    "        with open(os.path.join(sub_artist_folder, filename,), \"w\",encoding='utf-8') as f:\n",
    "            f.write(title + '\\n')\n",
    "            f.write(lyrics)\n",
    "            total_pages += 1\n",
    "            \n",
    "      \n",
    "            \n",
    "            \n",
    "            print(f'{total_pages} pages scraped')\n",
    "            end = time.time()\n",
    "            print(f'Elapsed time: {end - start}')\n",
    "            # Remember to pull at least 20 songs per artist. It may be fun to pull all the songs for the artist\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5186f0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 1.14 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931993f",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "\n",
    "This assignment asks you to pull data from the Twitter API and scrape www.AZLyrics.com. After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0e1538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00bc106",
   "metadata": {},
   "source": [
    "******\n",
    "\n",
    "**Checking Twitter Data**\n",
    "\n",
    "The output from your Twitter API pull should be two files per artist, stored in files with formats like cher_followers.txt (a list of all follower IDs you pulled) and cher_followers_data.txt. These files should be in a folder named twitter within the repository directory. This code summarizes the information at a high level to help the instructor evaluate your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce5c8475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see two artist handles: shakira and sanbenito.\n"
     ]
    }
   ],
   "source": [
    "twitter_files = os.listdir(\"twitter\")\n",
    "twitter_files = [f for f in twitter_files if f != \".DS_Store\"]\n",
    "artist_handles = list(set([name.split(\"_\")[0] for name in twitter_files]))\n",
    "\n",
    "print(f\"We see two artist handles: {artist_handles[0]} and {artist_handles[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da01ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see 100 in your follower file for shakira, assuming a header row.\n",
      "In the follower data file (shakira_id_follower_data.txt) for shakira, we have these columns:\n",
      "        screen_name                              name                   id                        location  followers_count  friends_count                                                                                                                                                      description\n",
      "\n",
      "We have 100 data rows for shakira in the follower data file.\n",
      "For shakira we have 100 unique locations.\n",
      "For shakira we have 100 words in the descriptions..\n",
      "Here are the five most common words:\n",
      "[('', 62), ('Artista visual', 1), ('BiÃ³loga, Magistra en GestiÃ³n Ambiental. Escribiendo historias.... Interesada en Cambio ClimÃ¡tico y Soluciones Basadas en Naturaleza', 1), ('Hola', 1), ('Every winter has its spring', 1)]\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "We see 100 in your follower file for sanbenito, assuming a header row.\n",
      "In the follower data file (sanbenito_id_follower_data.txt) for sanbenito, we have these columns:\n",
      "        screen_name                                 name                   id                       location  followers_count  friends_count                                                                                                                             description\n",
      "\n",
      "We have 100 data rows for sanbenito in the follower data file.\n",
      "For sanbenito we have 100 unique locations.\n",
      "For sanbenito we have 100 words in the descriptions..\n",
      "Here are the five most common words:\n",
      "[('', 62), ('Artista visual', 1), ('BiÃ³loga, Magistra en GestiÃ³n Ambiental. Escribiendo historias.... Interesada en Cambio ClimÃ¡tico y Soluciones Basadas en Naturaleza', 1), ('Hola', 1), ('Every winter has its spring', 1)]\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist in artist_handles :\n",
    "    follower_file = artist + \"_followers.txt\"\n",
    "    follower_data_file = artist + \"_id_follower_data.txt\"\n",
    "    \n",
    "    ids = open(\"twitter/\" + artist +\"/\"+ follower_file,'r').readlines()\n",
    "    \n",
    "    print(f\"We see {len(ids)-1} in your follower file for {artist}, assuming a header row.\")\n",
    "    \n",
    "    with open(\"twitter/\" + artist +\"/\"+ follower_data_file,'r', encoding='utf-8') as infile :\n",
    "        \n",
    "        # check the headers\n",
    "        headers = infile.readline().split(\"\\t\")\n",
    "        \n",
    "        print(f\"In the follower data file ({follower_data_file}) for {artist}, we have these columns:\")\n",
    "        print(\" : \".join(headers))\n",
    "        \n",
    "        description_words = []\n",
    "        locations = set()\n",
    "        \n",
    "        \n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            \n",
    "            try : \n",
    "                locations.add(line[3])            \n",
    "                description_words.extend(words(line[6]))\n",
    "            except :\n",
    "                pass\n",
    "    \n",
    "        \n",
    "\n",
    "        print(f\"We have {idx+1} data rows for {artist} in the follower data file.\")\n",
    "\n",
    "        #print(f\"For {artist} we have {len(locations)} unique locations.\")\n",
    "        #change the way location and description is checked\n",
    "        location_list = user_fields[\"location\"]\n",
    "        print(f\"For {artist} we have {len(location_list)} unique locations.\")\n",
    "        \n",
    "        #print(f\"For {artist} we have {len(description_words)} words in the descriptions.\")\n",
    "        description_words = user_fields[\"description\"]\n",
    "        print(f\"For {artist} we have {len(description_words)} words in the descriptions..\")\n",
    "\n",
    "       \n",
    "        print(\"Here are the five most common words:\")\n",
    "        print(Counter(description_words).most_common(5))\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a1ed9",
   "metadata": {},
   "source": [
    "******\n",
    "**Checking Lyrics**\n",
    "\n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory: /lyrics/[Artist Name]/[filename from URL]. This code summarizes the information at a high level to help the instructor evaluate your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6feedd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For badbunny we have 143 files.\n",
      "For badbunny we have roughly 81057 words, 7992 are unique.\n",
      "For shakira we have 157 files.\n",
      "For shakira we have roughly 94469 words, 8169 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name, encoding=\"utf8\") as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f379fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
